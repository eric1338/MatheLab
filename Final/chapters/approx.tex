\section{Function Approximation}


\subsection{Problem 5.9}


\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{chapters/images/desc-5-9}
\end{figure}


\subsubsection{a)}

To solve the linear equation for the coefficients the \textit{numpy} library can be used again.

\begin{lstlisting}[caption=Problem 5.9 a)]
def getCoefficients(fx, degree, a, b):
	xses = [];
	yses = [];
	
	step = (b - a) / float(degree - 1);
	
	for i in range(degree):
		x = a + i * step;
		xses.append(x);
		yses.append(fx(x));
	
	matA = [];
	
	for xVal in xses:
		row = [];
		for i in range(degree):
			row.append(pow(xVal, i));
		
		matA.append(row);
	
	matrixArr = np.array(matA);
	vectorArr = np.array(yses);
	
	linSolutions = np.linalg.solve(matrixArr, vectorArr);
	
	return linSolutions;
\end{lstlisting}

This function will be used in the following subtasks.


\subsubsection{b)}

To use the coefficients as a function, a python function for the calculation of the $y$ values has to be defined as well:

\begin{lstlisting}[caption=Function which uses the calculated coefficients]
def polynomialF(coefficients, x):
	sum = 0.0;
	
	for i in range(len(coefficients)):
		sum += coefficients[i] * pow(x, i);
	
	return sum;
\end{lstlisting}

Afterwards, the function values for the coefficients can be calculated and the result plotted.

\begin{lstlisting}[caption=Problem 5.9 b)]
def myF(x):
	return pow(math.e, -pow(x, 2));

coefficients = getCoefficients(myF, 10, -2, 10);

xses = [];
realYses = [];
approxYses = [];

for i in range(100):
	x = -2 + i * (12 / 100.0);
	
	xses.append(x);
	realYses.append(myF(x));
	approxYses.append(polynomialF(coefficients, x));

plt.plot(xses, realYses);
plt.plot(xses, approxYses);
plt.xlabel("x");
plt.ylabel("y");
plt.show();
\end{lstlisting}

The resulting graph looks like the following:

%\begin{figure}[h!]
\includegraphics[width=1\textwidth]{chapters/images/figure-5-9-b}
%\caption{Plot of the }
%\end{figure}

\begin{figure}[h!]
%\includegraphics[width=1\textwidth]{chapters/images/figure-5-9-b}
\caption{Plot of both functions}
\end{figure}

The blue curve represents the real values, while the green curve represents the graph of the polynomial approximation. Within the interval $[-1, 8]$ the deviations to be relatively small, outside of the interval however there seem to be bigger differences between both functions.


\subsubsection{c)}

\begin{lstlisting}[caption=Problem 5.9 c)]
maxApproxDev = -1;

for i in range(100):
	realY = realYses[i];
	approxY = approxYses[i];
	
	maxApproxDev = max(maxApproxDev, abs(realY - approxY));

print("maximum deviation: " + str(maxApproxDev));
\end{lstlisting}


The results of the calculation of the maximum deviation are:

\begin{lstlisting}[caption=Result of 5.9 c), keywordstyle=\color{black}]
maximum deviation: 0.634025332357
\end{lstlisting}


\subsubsection{d)}

First the functions to get the $y$ values for both Taylor series have to be defined:

\begin{lstlisting}[caption=Definition of the Taylor functions]
def taylorFuncAt0(x):
	sum = 1;
	sum -= pow(x, 2);
	sum += pow(x, 4) / 2.0;
	sum -= pow(x, 6) / 6.0;
	sum += pow(x, 8) / 24.0;
	sum -= pow(x, 10) / 120.0;
	sum += pow(x, 12) / 720.0;
	sum -= pow(x, 14) / 5040.0;
	sum += pow(x, 16) / 40320.0;
	
	return sum;

def taylorFuncAt4(x):
	ep16 = float(pow(math.e, 16));
	xm4 = x - 4;
	
	sum = 1 / ep16;
	sum -= 8 * xm4 / ep16;
	sum += 31 * pow(xm4, 2) / ep16;
	sum -= 232 * pow(xm4, 3) / (3 * ep16);
	sum += 835 * pow(xm4, 4) / (6 * ep16);
	sum -= 2876 * pow(xm4, 5) / (15 * ep16);
	sum += 18833 * pow(xm4, 6) / (90 * ep16);
	sum -= 58076 * pow(xm4, 7) / (315 * ep16);
	sum += 332777 * pow(xm4, 8) / (2520 * ep16);
	sum -= 43325 * pow(xm4, 9) / (567 * ep16);
	sum += 3937007 * pow(xm4, 10) / (113400 * ep16);
	
	return sum;

taylor0Yses = [];
taylor4Yses = [];

for i in range(100):
	x = -2 + i * (12 / 100.0);
	
	taylor0Yses.append(taylorFuncAt0(x));
	taylor4Yses.append(taylorFuncAt4(x));
\end{lstlisting}

Afterwards the deviation of both functions to the original function can be calculated:

\begin{lstlisting}[caption=Problem 5.9 d)]
maxTaylor0Dev = -1;
maxTaylor4Dev = -1;

for i in range(100):
	realY = realYses[i];
	taylor0Y = taylor0Yses[i];
	taylor4Y = taylor4Yses[i];
	
	maxTaylor0Dev = max(maxTaylor0Dev, abs(realY - taylor0Y));
	maxTaylor4Dev = max(maxTaylor4Dev, abs(realY - taylor4Y));

print("maximum deviation of taylor series with c = 0: " + str(maxTaylor0Dev));
print("maximum deviation of taylor series with c = 4: " + str(maxTaylor4Dev));
\end{lstlisting}

The results of the calculation are:

\begin{lstlisting}[caption=Result of 5.9 d), keywordstyle=\color{black}]
maximum deviation of taylor series with c = 0: 1.88827128486e+11
maximum deviation of taylor series with c = 4: 354.936464804
\end{lstlisting}

As it can be easily seen, the Taylor series expanded around $x_{0} = 0$ has a much bigger deviation than the series expanded around $x_{0} = 4$. The reason for that is that the $y$ value of a Taylor series tends to deviate stronger as further the respective $x$ is from the expantion point $x_{0}$. With $x_{0} = 4$ the maximum $\Delta x$ ($\Delta x = |x - x_{0}|$) is 6, which is as little as possible since $4$ is in the middle of the interval $[-2, 10]$. With $x_{0} = 0$ however the maximum $\Delta x$ is 10, therefore the Taylor series expanded there is deviating much more.


\newpage

\subsection{Problem 5.10}

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{chapters/images/desc-5-10}
\end{figure}


\subsubsection{a)}

\begin{lstlisting}[caption=Problem 5.10 a)]
def getCoefficients(baseFunctions, dataPoints):
	l = len(baseFunctions);
	
	matA = [];
	
	for i in range(l):
		matrixRow = [];
		for j in range(l):
			elemSum = 0;
		
			for dp in dataPoints:
				x = dp[0];
				elemSum += baseFunctions[i](x) * baseFunctions[j](x);
			
			matrixRow.append(elemSum);
		
		matA.append(matrixRow);
	
	vecB = [];
	
	for i in range(l):
		elemSum = 0;
	
		for dp in dataPoints:
			elemSum += dp[1] * baseFunctions[i](dp[0]);
		
		vecB.append(elemSum);
	
	matrixArr = np.array(matA);
	vectorArr = np.array(vecB);
	
	linSolutions = np.linalg.solve(matrixArr, vectorArr);
	
	return linSolutions;
\end{lstlisting}


\subsubsection{b)}

Wth the function from a), the linear equations can be easily approximated by using two base functions:

\begin{lstlisting}[caption=Problem 5.10 b)]
def myF1(x): return x;
def myF0(x): return 1;

baseFns = [];

baseFns.append(myF1);
baseFns.append(myF0);

linPoints = [];
rndPoints = [];
xses = [];
yses = [];
rndYses = [];

for i in range(100):
	x = i / 10.0;
	y = x * 0.64 + 2.3;
	
	xses.append(x);
	yses.append(y);

	rnd = random.random() - 0.5;
	rndY = y + rnd * 0.6;
	rndYses.append(rndY);
	
	linPoints.append([x, y]);
	rndPoints.append([x, rndY]);

print("coefficients:");
print(getCoefficients(baseFns, linPoints));

print("approximated coefficients:");
print(getCoefficients(baseFns, rndPoints));

plt.plot(xses, yses);
plt.plot(xses, rndYses, "ro");
plt.xlabel("x");
plt.ylabel("y");
plt.show();
\end{lstlisting}

The coefficients calculated from the random data points are the following:

\begin{lstlisting}[caption=Result of 5.10 b), keywordstyle=\color{black}]
coefficients:
[ 0.64   0.23 ]

approximated coefficients:
[ 0.63139257   2.33442881 ]
\end{lstlisting}

As it can be seen, the approximated coefficients deviate very little from the actual linear equation.

The exact linear equation and the data points are visualized in the following graph:

\newpage

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{chapters/images/figure-5-10-b}
\caption{The linear equation as well as the data points}
\end{figure}


\subsubsection{c)}

Using the function from a), new data points and new base function, a polynomial of degree 4 (as well as degree 2 for the subtask d)) can be easily created:

\begin{lstlisting}[caption=Problem 5.10 c)]
def myF4(x): return pow(x, 4);
def myF3(x): return pow(x, 3);
def myF2(x): return pow(x, 2);
def myF1(x): return x;
def myF0(x): return 1;

def polynomialF(coefficients, x):
	sum = 0.0;
	
	nc = len(coefficients);
	
	for i in range(nc):
		sum += coefficients[i] * pow(x, nc - i - 1);
	
	return sum;

txtBaseFns4 = [];
txtBaseFns2 = [];

txtBaseFns4.append(myF4);
txtBaseFns4.append(myF3);
txtBaseFns4.append(myF2);
txtBaseFns4.append(myF1);
txtBaseFns4.append(myF0);

txtBaseFns2.append(myF2);
txtBaseFns2.append(myF1);
txtBaseFns2.append(myF0);

txtPoints = [];

txtPoints.append([8, -16186.1]);
txtPoints.append([9, -2810.82]);
# [...] (I left out the other values in the paper to save space)
txtPoints.append([45, -17641.9]);
txtPoints.append([46, -37150.2]);

txtCoefficients4 = getCoefficients(txtBaseFns4, txtPoints);
txtCoefficients2 = getCoefficients(txtBaseFns2, txtPoints);

txtPointsXses = [];
txtPointsYses = [];
\end{lstlisting}


\subsubsection{d)}

With both polynomial functions the squared error can now be calculated:

\begin{lstlisting}[caption=Calculating the squared errors of both polynomials]
error4 = 0;
error2 = 0;

for i in range(len(txtPoints)):
	txtPoint = txtPoints[i];
	
	txtPointsXses.append(txtPoint[0]);
	txtPointsYses.append(txtPoint[1]);
	
	y4 = polynomialF(txtCoefficients4, i + 8);
	y2 = polynomialF(txtCoefficients2, i + 8);
	
	error4 += pow(y4 - txtPoint[1], 2);
	error2 += pow(y2 - txtPoint[1], 2);

print("error of degree 4 polynomial: " + str(error4));
print("error of degree 2 polynomial: " + str(error2));
\end{lstlisting}

The resulting errors are:

\begin{lstlisting}[caption=Squared errors of both polynomials, keywordstyle=\color{black}]
error of degree 4 polynomial: 101457690.277
error of degree 2 polynomial: 7711489909.2
\end{lstlisting}

As it can be seen, the error of the degree 4 polynomial is over 70 times smaller than the degree 2 polynomial. This is mainly due to the fact that the data points lay suitable for a degree 4 polynomial.

\begin{lstlisting}[caption=Plotting both polynomials]
for i in range(152):
	x = 8 + i * 0.25;
	
	txtFXses.append(x);
	
	y4 = polynomialF(txtCoefficients4, x);
	y2 = polynomialF(txtCoefficients2, x);
	
	txtF4Yses.append(y4);
	txtF2Yses.append(y2);

plt.plot(txtFXses, txtF4Yses);
plt.plot(txtFXses, txtF2Yses);
plt.plot(txtPointsXses, txtPointsYses, "ro");
plt.xlabel("x");
plt.ylabel("y");
plt.show();
\end{lstlisting}

The plot of the polynomials look like this:

\newpage

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{chapters/images/figure-5-10-d}
\caption{Result of Problem 5.10 d)}
\end{figure}


\subsection{Problem 5.11}


\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{chapters/images/desc-5-11}
\end{figure}

The previously defined functions $getCoefficients$ and $polynomialF$ as well as the data points $txtPoints$ from problem 5.10 will be used in this exercise too.


\subsubsection{a)}

\begin{lstlisting}[caption=Problem 5.11 a)]
evenPoints = [];

for txtPoint in txtPoints:
	if txtPoint[0] % 2 == 0:
		evenPoints.append(txtPoint);
\end{lstlisting}


\subsubsection{b)}

\begin{lstlisting}[caption=Problem 5.11 b)]
matrixX = [];
matrixT = [];

for evenPoint in evenPoints:
	rnd = random.random();
	
	if len(matrixT) >= 10 or (rnd < 0.5 and len(matrixX) < 10):
		matrixX.append(evenPoint);
	else:
		matrixT.append(evenPoint);
\end{lstlisting}


\subsubsection{c)}

To get the polynomials up to degree 8, first the base functions have to be defined:

\begin{lstlisting}[caption=Base functions up to degree 8]
def myF8(x): return pow(x, 8);
def myF7(x): return pow(x, 7);
def myF6(x): return pow(x, 6);
def myF5(x): return pow(x, 5);
def myF4(x): return pow(x, 4);
def myF3(x): return pow(x, 3);
def myF2(x): return pow(x, 2);
def myF1(x): return x;
def myF0(x): return 1;

def getBaseFunctionList(degree):
	baseFunctions = [];
	
	if degree >= 8: baseFunctions.append(myF8);
	if degree >= 7: baseFunctions.append(myF7);
	if degree >= 6: baseFunctions.append(myF6);
	if degree >= 5: baseFunctions.append(myF5);
	if degree >= 4: baseFunctions.append(myF4);
	if degree >= 3: baseFunctions.append(myF3);
	if degree >= 2: baseFunctions.append(myF2);
	
	baseFunctions.append(myF1);
	baseFunctions.append(myF0);
	
	return baseFunctions;
\end{lstlisting}

By using these base functions the coefficients can be easily calculated with the $getCoefficients$ function. Afterwards, the $y$ values of those functions can be calculated with the $polynomialF$ function in order to plot them.

\begin{lstlisting}[caption=Problem 5.11 c)]
xses = [];
allYses = [];

for i in range(152):
	xses.append(8 + i * 0.25);

for i in range(7):
	degree = i + 2;
	
	baseFunctionList = getBaseFunctionList(degree);
	
	coefficients = getCoefficients(baseFunctionList, matrixX);
	
	yses = [];
	
	for x in xses:
		y = polynomialF(coefficients, x);
		yses.append(y);
	
	allYses.append(yses);

xXses = [];
xYses = [];
tXses = [];
tYses = [];

for i in range(10):
	xXses.append(matrixX[i][0]);
	xYses.append(matrixX[i][1]);
	tXses.append(matrixT[i][0]);
	tYses.append(matrixT[i][1]);

colors = ["b", "c", "r", "g", "m", "y", "k"];

plt.plot(xXses, xYses, "bo");
plt.plot(tXses, tYses, "ro");

for i in range(len(allYses)):
	plt.plot(xses, allYses[i], colors[i]);

plt.xlabel("x");
plt.ylabel("y");
plt.show();
\end{lstlisting}

The resulting graph looks like this.

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{chapters/images/figure-5-11-c}
\caption{All 7 polynomials}
\end{figure}


\subsubsection{d)}

\begin{lstlisting}[caption=Problem 5.11 d)]
def getError(coefficients, mat):
	errorSum = 0;
	
	for elem in mat:
		fY = polynomialF(coefficients, elem[0]);
		
		errorSum += pow(fY - elem[1], 2);
	
	return errorSum;

def getErrors(degree, matX, matT):
	baseFunctionList = getBaseFunctionList(degree);
	
	coefficients = getCoefficients(baseFunctionList, matX);
	
	errorSumX = getError(coefficients, matX);
	errorSumT = getError(coefficients, matT);
	
	return [errorSumX, errorSumT];

degrees = [];
errorsX = [];
errorsT = [];

for i in range(7):
	degree = i + 2;
	degrees.append(degree);
	
	errors = getErrors(degree, matrixX, matrixT);
	
	errorsX.append(errors[0]);
	errorsT.append(errors[1]);

plt.plot(degrees, errorsX, "b");
plt.plot(degrees, errorsT, "r");

plt.xlabel("degree");
plt.ylabel("error");
plt.show();
\end{lstlisting}

The resulting errors look the like the following:

\newpage

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{chapters/images/figure-5-11-d}
\caption{The errors of the polynomials}
\end{figure}


\subsubsection{e)}

The deviation to the original approximated function (where all data points were used) minimizes the more evenly the points of X and T are distributed. The reason for that is that when there's a large interval of test points only, the approximated function has no bounds at all in that interval and can deviate as much as it needs to, without increasing the error.

For the training data X the error decreases the higher the degree of the polynomial is. For the test data T however the error first decreases up degree 4 and 5, but then 
rapidly increases with each additional degree. A reason for that could be that functions of higher degrees tend to have bigger distortions, and since there are no bounds on the test data points they are free to do so.

In most of the cases the polynomials of degree 4 and 5 had the best results.


\subsubsection{f)}

It makes sense because the approximated function is supposed to be close to all data values, even the points \enquote{in between} which are not part of both the training and test data.